{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbabc74b",
   "metadata": {},
   "source": [
    "# ArXiv Semantic Graph – Project Demo\n",
    "\n",
    "This notebook demonstrates the main steps of our pipeline:\n",
    "\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Embeddings (Universal Sentence Encoder)\n",
    "3. HNSW index for fast nearest neighbours\n",
    "4. Distance histogram and global threshold τ\n",
    "5. Semantic graph construction\n",
    "6. Graph clustering (Louvain communities)\n",
    "7. Semantic recommendation demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b4013",
   "metadata": {},
   "source": [
    "## Section 0 - Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60c8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/zealot/DTU/Computational_Tools\n",
      "SRC: /home/zealot/DTU/Computational_Tools/src\n",
      "\n",
      "PYTHONPATH updated. Current sys.path first entries:\n",
      "/home/zealot/micromamba/envs/arxiv/lib/python311.zip\n",
      "/home/zealot/micromamba/envs/arxiv/lib/python3.11\n",
      "/home/zealot/micromamba/envs/arxiv/lib/python3.11/lib-dynload\n",
      "\n",
      "[OK] Project modules imported successfully.\n",
      "Embeddings directory not found at:\n",
      "/home/zealot/DTU/Computational_Tools/outputs/embeddings\n",
      "Run the embedding extraction pipeline first.\n",
      "No embedding shards found in /home/zealot/DTU/Computational_Tools/outputs/embeddings\n",
      "\n",
      "[Info] Found 0 embedding shards.\n"
     ]
    }
   ],
   "source": [
    "# Always reload local project modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/zealot/DTU/Computational_Tools/src\")\n",
    "\n",
    "# ============================\n",
    "# SECTION 0 — PROJECT SETUP\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# -----------------------------\n",
    "# 0.1 — Resolve project paths\n",
    "# -----------------------------\n",
    "# Your repo structure:\n",
    "# project/\n",
    "#   ├─ src/\n",
    "#   ├─ outputs/\n",
    "#   ├─ notebooks/\n",
    "#   └─ data/\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT = NOTEBOOK_DIR.parent\n",
    "SRC = ROOT / \"src\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SRC:\", SRC)\n",
    "\n",
    "# -----------------------------\n",
    "# 0.2 — Add src/ to PYTHONPATH\n",
    "# -----------------------------\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "print(\"\\nPYTHONPATH updated. Current sys.path first entries:\")\n",
    "print(\"\\n\".join(sys.path[:3]))\n",
    "\n",
    "# -----------------------------\n",
    "# 0.3 — Project imports\n",
    "# -----------------------------\n",
    "from arxiv_semantic_graph import eda, embeddings, graph, graph_clustering, recommend\n",
    "\n",
    "print(\"\\n[OK] Project modules imported successfully.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 0.4 — Paths to data and outputs\n",
    "# -----------------------------\n",
    "DATA_PATH = ROOT / \"data\" / \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "OUTPUTS = ROOT / \"outputs\"\n",
    "OUTPUTS.mkdir(exist_ok=True)\n",
    "\n",
    "EMB_DIR = OUTPUTS / \"embeddings\"\n",
    "if not EMB_DIR.exists():\n",
    "    print(\n",
    "        f\"Embeddings directory not found at:\\n{EMB_DIR}\\n\"\n",
    "        \"Run the embedding extraction pipeline first.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 0.5 — Locate embedding shards\n",
    "# -----------------------------\n",
    "emb_paths = sorted(EMB_DIR.glob(\"emb_*.npy\"))\n",
    "if not emb_paths:\n",
    "    print(f\"No embedding shards found in {EMB_DIR}\")\n",
    "\n",
    "print(f\"\\n[Info] Found {len(emb_paths)} embedding shards.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262194c",
   "metadata": {},
   "source": [
    "## Section 1 - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661a6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "years, lengths = eda.load_eda_data(\n",
    "    file_path=str(DATA_PATH),\n",
    "    max_papers=50000,\n",
    "    min_year=2010,\n",
    ")\n",
    "\n",
    "stats = eda.compute_eda_stats(years, lengths)\n",
    "\n",
    "eda.plot_year_distribution(years, save_path=None, show=True)\n",
    "eda.plot_length_distribution(lengths, save_path=None, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9250b53",
   "metadata": {},
   "source": [
    "## Section 2 -Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da57ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zealot/micromamba/envs/arxiv/bin/python\n",
      "2.15.0\n",
      "True\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import sys, tensorflow as tf\n",
    "print(sys.executable)\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a59eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CPU] Running without GPU (or no GPU detected)\n",
      "[Embeddings] Reading from: ../data/arxiv-metadata-oai-snapshot.json\n",
      "[Embeddings] Writing shards to: ../outputs/embeddings\n",
      "[Embeddings] Filters: min_year=2024, min_words=200\n",
      "[Embeddings] Batch size: 64, Shard size: 25000\n",
      "[USE] Loading Universal Sentence Encoder...\n",
      "[USE] Model loaded successfully\n",
      "[Embeddings] Starting streaming and embedding...\n",
      "[Shard 0] saved 25000 papers (total 25000)\n",
      "[Shard 1] saved 25000 papers (total 50000)\n",
      "[Shard 2] saved 25000 papers (total 75000)\n",
      "[Shard 3] saved 25000 papers (total 100000)\n",
      "[Shard 4] saved 25000 papers (total 125000)\n",
      "[Shard 5] saved 23477 papers (total 148477)\n",
      "[Embeddings] Complete! Total papers embedded: 148,477\n",
      "[Embeddings] Verification: 6 shards, 148,477 vectors, dim=512\n",
      "Total embedded: 148477\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(\"../data/arxiv-metadata-oai-snapshot.json\")\n",
    "EMB_DIR = Path(\"../outputs/embeddings\")\n",
    "EMB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "total_embedded = embeddings.run_embeddings(\n",
    "    file_path=str(DATA_PATH),\n",
    "    out_dir=str(EMB_DIR),\n",
    "    batch_size=64,\n",
    "    shard_size=25000,\n",
    "    min_year=2024,\n",
    "    min_words=200,\n",
    "    use_gpu=True,           # ← ENABLE GPU\n",
    "    vram_mib=4096,          # ← OPTIONAL: cap GPU memory\n",
    "    force_recompute=False,\n",
    ")\n",
    "\n",
    "print(\"Total embedded:\", total_embedded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77708c5",
   "metadata": {},
   "source": [
    "## Section 3 - HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f843e7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Total vectors: 148,477 | Dimension: 512\n",
      "[HNSW] Building index…\n",
      "  Added 25000/148477\n",
      "  Added 50000/148477\n",
      "  Added 75000/148477\n",
      "  Added 100000/148477\n",
      "  Added 125000/148477\n",
      "  Added 148477/148477\n",
      "[HNSW] Index is ready.\n",
      "[HNSW] Example query for id 85714 returns 5 neighbors.\n",
      "Neighbor IDs: [85714 95095 41168  7215 99717]\n",
      "Distances:    [8.9406967e-07 2.2669423e-01 2.2755516e-01 2.4245679e-01 2.4412102e-01]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import importlib\n",
    "import arxiv_semantic_graph.graph as g\n",
    "\n",
    "importlib.reload(g)\n",
    "from arxiv_semantic_graph.graph import load_shapes, build_or_load_index\n",
    "\n",
    "\n",
    "# 3.1 – Locate embedding shards\n",
    "emb_paths = sorted(glob.glob(str(EMB_DIR / \"emb_*.npy\")))\n",
    "assert emb_paths, f\"No emb_*.npy files found in {EMB_DIR}\"\n",
    "\n",
    "N, D = load_shapes(emb_paths)\n",
    "print(f\"[Info] Total vectors: {N:,} | Dimension: {D}\")\n",
    "\n",
    "# 3.2 – Build or load HNSW index\n",
    "index = build_or_load_index(\n",
    "    emb_paths=emb_paths,\n",
    "    dim=D,\n",
    "    out_dir=OUTPUTS / \"hnsw\",\n",
    "    efc=200,   # ef_construction (controls construction quality vs speed)\n",
    "    M=16,      # maximum number of connections per node\n",
    "    threads=8,\n",
    ")\n",
    "\n",
    "# 3.3 – Quick sanity check: k-NN query for a random paper\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random_id = random.randint(0, N - 1)\n",
    "\n",
    "# Load the actual embedding for that paper (better than using a dummy vector)\n",
    "offset = 0\n",
    "query_vec = None\n",
    "for p in emb_paths:\n",
    "    arr = np.load(p, mmap_mode=\"r\")\n",
    "    n = arr.shape[0]\n",
    "    if random_id < offset + n:\n",
    "        query_vec = arr[random_id - offset].astype(\"float32\")\n",
    "        break\n",
    "    offset += n\n",
    "\n",
    "assert query_vec is not None, \"Could not locate embedding for random_id\"\n",
    "\n",
    "labels, dists = index.knn_query(query_vec.reshape(1, -1), k=5)\n",
    "\n",
    "print(f\"[HNSW] Index is ready.\")\n",
    "print(f\"[HNSW] Example query for id {random_id} returns {labels.shape[1]} neighbors.\")\n",
    "print(\"Neighbor IDs:\", labels[0])\n",
    "print(\"Distances:   \", dists[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72f3f3",
   "metadata": {},
   "source": [
    "## Section 4 — Distance histogram & τ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9776e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Section 3] Building or loading HNSW index...\n",
      "[Info] Total vectors = 148,477 | Dimension = 512\n",
      "[HNSW] Loaded existing index.\n",
      "[Section 3] HNSW index ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Section 3 — HNSW Index\n",
    "# ---------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "from arxiv_semantic_graph.graph import load_shapes, build_or_load_index\n",
    "\n",
    "print(\"[Section 3] Building or loading HNSW index...\")\n",
    "\n",
    "# 3.1 Locate embedding shards\n",
    "EMB_DIR = ROOT / \"outputs\" / \"embeddings\"\n",
    "emb_paths = sorted(glob.glob(str(EMB_DIR / \"emb_*.npy\")))\n",
    "assert emb_paths, f\"No embedding shards found in: {EMB_DIR}\"\n",
    "\n",
    "# 3.2 Get total vectors (N) and dimensionality (D)\n",
    "N, D = load_shapes(emb_paths)\n",
    "print(f\"[Info] Total vectors = {N:,} | Dimension = {D}\")\n",
    "\n",
    "# 3.3 Build or load HNSW index\n",
    "index = build_or_load_index(\n",
    "    emb_paths=emb_paths,\n",
    "    dim=D,\n",
    "    out_dir=OUTPUTS / \"hnsw\",\n",
    "    efc=200,\n",
    "    M=16,\n",
    "    threads=8,\n",
    ")\n",
    "\n",
    "print(\"[Section 3] HNSW index ready.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf888cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Section 4] Computing k-NN distance histogram...\n",
      "[Histogram] Computing distance distributions with k=6…\n",
      "  Processed 100000 vectors\n",
      "[Histogram] Statistics: {\n",
      "  \"mean\": 0.25393904993931454,\n",
      "  \"median\": 0.25344739854335785,\n",
      "  \"std\": 0.0445670277501146,\n",
      "  \"min\": -1.1920928955078125e-07,\n",
      "  \"max\": 0.5962079763412476\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.25393904993931454,\n",
       " 'median': 0.25344739854335785,\n",
       " 'std': 0.0445670277501146,\n",
       " 'min': -1.1920928955078125e-07,\n",
       " 'max': 0.5962079763412476}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arxiv_semantic_graph.graph import (\n",
    "    compute_knn_distance_histogram,\n",
    "    choose_tau_from_percentile,\n",
    ")\n",
    "\n",
    "print(\"[Section 4] Computing k-NN distance histogram...\")\n",
    "\n",
    "hist_result = compute_knn_distance_histogram(\n",
    "    emb_dir=str(EMB_DIR),\n",
    "    index=index,\n",
    "    k=6,                      # 1 (self) + 5 neighbors\n",
    "    out_dir=str(OUTPUTS / \"eda\")   # Stores plots & stats\n",
    ")\n",
    "\n",
    "hist_summary = hist_result[\"summary\"]\n",
    "hist_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19421480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "bins = hist_result[\"bins\"]\n",
    "global_hist = hist_result[\"global_hist\"]\n",
    "centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(centers, global_hist, width=(bins[1] - bins[0]), color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Cosine distance to nearest neighbors\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Global k-NN Distance Distribution\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557ffbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Threshold] tau ≈ 0.1900 (≈5th percentile)\n",
      "[Threshold] tau ≈ 0.2100 (≈10th percentile)\n",
      "[Threshold] tau ≈ 0.2200 (≈15th percentile)\n",
      "[Threshold] tau ≈ 0.2200 (≈20th percentile)\n",
      "[Threshold] tau ≈ 0.2300 (≈25th percentile)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.05, 0.19), (0.1, 0.21), (0.15, 0.22), (0.2, 0.22), (0.25, 0.23)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define candidate p_keep values\n",
    "pkeep_candidates = [0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "\n",
    "tau_candidates = []\n",
    "\n",
    "for p in pkeep_candidates:\n",
    "    tau = choose_tau_from_percentile(\n",
    "        hist_result[\"bins\"],\n",
    "        hist_result[\"global_hist\"],\n",
    "        pkeep=p\n",
    "    )\n",
    "    tau_candidates.append((p, tau))\n",
    "\n",
    "tau_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3e9abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved τ candidates to: /home/zealot/DTU/Computational_Tools/outputs/tau_candidates.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_keep</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_keep   tau\n",
       "0    0.05  0.19\n",
       "1    0.10  0.21\n",
       "2    0.15  0.22\n",
       "3    0.20  0.22\n",
       "4    0.25  0.23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tau_df = pd.DataFrame(\n",
    "    [{\"p_keep\": p, \"tau\": tau} for p, tau in tau_candidates]\n",
    ")\n",
    "\n",
    "tau_file = OUTPUTS / \"tau_candidates.csv\"\n",
    "tau_df.to_csv(tau_file, index=False)\n",
    "\n",
    "print(\"Saved τ candidates to:\", tau_file)\n",
    "\n",
    "tau_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606a908",
   "metadata": {},
   "source": [
    "## Section 5 — Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c325bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Setup] Found 6 shards: 148,477 vectors, dim=512\n"
     ]
    }
   ],
   "source": [
    "from arxiv_semantic_graph.graph import load_shapes, build_or_load_index\n",
    "# Locate embedding shards (they must already exist)\n",
    "emb_paths = sorted(glob.glob(str(EMB_DIR / \"emb_*.npy\")))\n",
    "if not emb_paths:\n",
    "    raise RuntimeError(\n",
    "        f\"No embedding shards found in {EMB_DIR}. \"\n",
    "        \"Run the embedding extraction pipeline before this notebook.\"\n",
    "    )\n",
    "\n",
    "# Basic embedding stats\n",
    "N, D = load_shapes(emb_paths)\n",
    "print(f\"[Setup] Found {len(emb_paths)} shards: {N:,} vectors, dim={D}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0149be69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_keep</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_keep   tau\n",
       "0    0.05  0.19\n",
       "1    0.10  0.21\n",
       "2    0.15  0.22\n",
       "3    0.20  0.22\n",
       "4    0.25  0.23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Info] Loaded tau candidates:\n",
      "  pkeep=0.05 → tau=0.1900\n",
      "  pkeep=0.10 → tau=0.2100\n",
      "  pkeep=0.15 → tau=0.2200\n",
      "  pkeep=0.20 → tau=0.2200\n",
      "  pkeep=0.25 → tau=0.2300\n",
      "[HNSW] Loaded existing index.\n",
      "\n",
      "[Graph] Starting graph construction for each tau...\n",
      "\n",
      "================================================================================\n",
      "[Graph] Building graph for pkeep=0.05, tau≈0.1900\n",
      "[Graph] Building edge list at tau=0.1900, k=50\n",
      "  Processed nodes 0-20000\n",
      "  Processed nodes 20000-25000\n",
      "  Processed nodes 25000-45000\n",
      "  Processed nodes 45000-50000\n",
      "  Processed nodes 50000-70000\n",
      "  Processed nodes 70000-75000\n",
      "  Processed nodes 75000-95000\n",
      "  Processed nodes 95000-100000\n",
      "  Processed nodes 100000-120000\n",
      "  Processed nodes 120000-125000\n",
      "  Processed nodes 125000-145000\n",
      "  Processed nodes 145000-148477\n",
      "[Graph] Edge list written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/edges_tau0.190.tsv\n",
      "[Graph] Sample edges with titles -> /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/arxiv_edge_list_sample.txt\n",
      "[Stats] Computing graph statistics for /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/edges_tau0.190.tsv\n",
      "[Stats] Written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/arxiv_graph_stats.txt\n",
      "[Stats] {\n",
      "  \"nodes\": 148477,\n",
      "  \"edges\": 25347,\n",
      "  \"avg_degree\": 0.34142661826410825,\n",
      "  \"median_degree\": 1.0,\n",
      "  \"isolated_nodes\": 129176\n",
      "}\n",
      "================================================================================\n",
      "[Graph] Building graph for pkeep=0.10, tau≈0.2100\n",
      "[Graph] Building edge list at tau=0.2100, k=50\n",
      "  Processed nodes 0-20000\n",
      "  Processed nodes 20000-25000\n",
      "  Processed nodes 25000-45000\n",
      "  Processed nodes 45000-50000\n",
      "  Processed nodes 50000-70000\n",
      "  Processed nodes 70000-75000\n",
      "  Processed nodes 75000-95000\n",
      "  Processed nodes 95000-100000\n",
      "  Processed nodes 100000-120000\n",
      "  Processed nodes 120000-125000\n",
      "  Processed nodes 125000-145000\n",
      "  Processed nodes 145000-148477\n",
      "[Graph] Edge list written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/edges_tau0.210.tsv\n",
      "[Graph] Sample edges with titles -> /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/arxiv_edge_list_sample.txt\n",
      "[Stats] Computing graph statistics for /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/edges_tau0.210.tsv\n",
      "[Stats] Written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/arxiv_graph_stats.txt\n",
      "[Stats] {\n",
      "  \"nodes\": 148477,\n",
      "  \"edges\": 101171,\n",
      "  \"avg_degree\": 1.3627834614115317,\n",
      "  \"median_degree\": 2.0,\n",
      "  \"isolated_nodes\": 106717\n",
      "}\n",
      "================================================================================\n",
      "[Graph] Building graph for pkeep=0.15, tau≈0.2200\n",
      "[Graph] Building edge list at tau=0.2200, k=50\n",
      "  Processed nodes 0-20000\n",
      "  Processed nodes 20000-25000\n",
      "  Processed nodes 25000-45000\n",
      "  Processed nodes 45000-50000\n",
      "  Processed nodes 50000-70000\n",
      "  Processed nodes 70000-75000\n",
      "  Processed nodes 75000-95000\n",
      "  Processed nodes 95000-100000\n",
      "  Processed nodes 100000-120000\n",
      "  Processed nodes 120000-125000\n",
      "  Processed nodes 125000-145000\n",
      "  Processed nodes 145000-148477\n",
      "[Graph] Edge list written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/edges_tau0.220.tsv\n",
      "[Graph] Sample edges with titles -> /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/arxiv_edge_list_sample.txt\n",
      "[Stats] Computing graph statistics for /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/edges_tau0.220.tsv\n",
      "[Stats] Written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/arxiv_graph_stats.txt\n",
      "[Stats] {\n",
      "  \"nodes\": 148477,\n",
      "  \"edges\": 186603,\n",
      "  \"avg_degree\": 2.5135610229193746,\n",
      "  \"median_degree\": 2.0,\n",
      "  \"isolated_nodes\": 92679\n",
      "}\n",
      "[Graph] Skipping pkeep=0.20: tau=0.22 already used.\n",
      "================================================================================\n",
      "[Graph] Building graph for pkeep=0.25, tau≈0.2300\n",
      "[Graph] Building edge list at tau=0.2300, k=50\n",
      "  Processed nodes 0-20000\n",
      "  Processed nodes 20000-25000\n",
      "  Processed nodes 25000-45000\n",
      "  Processed nodes 45000-50000\n",
      "  Processed nodes 50000-70000\n",
      "  Processed nodes 70000-75000\n",
      "  Processed nodes 75000-95000\n",
      "  Processed nodes 95000-100000\n",
      "  Processed nodes 100000-120000\n",
      "  Processed nodes 120000-125000\n",
      "  Processed nodes 125000-145000\n",
      "  Processed nodes 145000-148477\n",
      "[Graph] Edge list written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/edges_tau0.230.tsv\n",
      "[Graph] Sample edges with titles -> /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/arxiv_edge_list_sample.txt\n",
      "[Stats] Computing graph statistics for /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/edges_tau0.230.tsv\n",
      "[Stats] Written to /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/arxiv_graph_stats.txt\n",
      "[Stats] {\n",
      "  \"nodes\": 148477,\n",
      "  \"edges\": 321786,\n",
      "  \"avg_degree\": 4.334489516894872,\n",
      "  \"median_degree\": 3.0,\n",
      "  \"isolated_nodes\": 77706\n",
      "}\n",
      "\n",
      "[Graph] Summary of constructed graphs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkeep</th>\n",
       "      <th>tau</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>isolated_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>148477</td>\n",
       "      <td>25347</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>148477</td>\n",
       "      <td>101171</td>\n",
       "      <td>1.362783</td>\n",
       "      <td>2.0</td>\n",
       "      <td>106717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>148477</td>\n",
       "      <td>186603</td>\n",
       "      <td>2.513561</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>148477</td>\n",
       "      <td>321786</td>\n",
       "      <td>4.334490</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pkeep   tau   nodes   edges  avg_degree  median_degree  isolated_nodes\n",
       "0   0.05  0.19  148477   25347    0.341427            1.0          129176\n",
       "1   0.10  0.21  148477  101171    1.362783            2.0          106717\n",
       "2   0.15  0.22  148477  186603    2.513561            2.0           92679\n",
       "3   0.25  0.23  148477  321786    4.334490            3.0           77706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph] Saved summary to: /home/zealot/DTU/Computational_Tools/outputs/graphs/graph_summaries.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Section 5 — Build and evaluate graphs for multiple τ thresholds\n",
    "# ==============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from arxiv_semantic_graph.graph import (\n",
    "    build_graph_for_tau,\n",
    "    compute_graph_stats,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5.0 – Load tau candidates from the saved CSV\n",
    "# ----------------------------------------------\n",
    "\n",
    "TAU_CSV = OUTPUTS / \"tau_candidates.csv\"\n",
    "\n",
    "if not TAU_CSV.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Tau candidate file not found at {TAU_CSV}. \"\n",
    "        \"Run Section 4 first to compute histogram & tau values.\"\n",
    "    )\n",
    "\n",
    "tau_df = pd.read_csv(TAU_CSV)\n",
    "display(tau_df)\n",
    "\n",
    "# Convert DF into list of (pkeep, tau)\n",
    "tau_candidates = list(zip(tau_df[\"p_keep\"], tau_df[\"tau\"]))\n",
    "print(\"\\n[Info] Loaded tau candidates:\")\n",
    "for p, t in tau_candidates:\n",
    "    print(f\"  pkeep={p:.2f} → tau={t:.4f}\")\n",
    "\n",
    "index = build_or_load_index(\n",
    "    emb_paths=emb_paths,\n",
    "    dim=D,\n",
    "    out_dir=OUTPUTS / \"hnsw\",\n",
    "    efc=200,   # ef_construction (controls construction quality vs speed)\n",
    "    M=16,      # maximum number of connections per node\n",
    "    threads=8,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5.1 – Prepare output directory for graph files\n",
    "# ----------------------------------------------\n",
    "\n",
    "GRAPH_DIR = OUTPUTS / \"graphs\"\n",
    "GRAPH_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "graph_summaries = []\n",
    "\n",
    "print(\"\\n[Graph] Starting graph construction for each tau...\\n\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5.2 – Build graphs for each τ + compute their metrics\n",
    "# -----------------------------------------------------\n",
    "\n",
    "seen_taus = set()\n",
    "graph_summaries = []\n",
    "\n",
    "for pkeep, tau in tau_candidates:\n",
    "    tau_rounded = round(tau, 3)\n",
    "    if tau_rounded in seen_taus:\n",
    "        print(f\"[Graph] Skipping pkeep={pkeep:.2f}: tau={tau_rounded} already used.\")\n",
    "        continue\n",
    "    seen_taus.add(tau_rounded)\n",
    "\n",
    "    tau_tag = f\"tau_{tau_rounded:.3f}\".replace(\".\", \"p\")\n",
    "    out_dir = GRAPH_DIR / tau_tag\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"[Graph] Building graph for pkeep={pkeep:.2f}, tau≈{tau_rounded:.4f}\")\n",
    "\n",
    "    edge_path = build_graph_for_tau(\n",
    "        emb_dir=str(EMB_DIR),\n",
    "        index=index,\n",
    "        tau=tau,\n",
    "        k_for_search=50,          # Number of neighbors to retrieve\n",
    "        out_dir=str(out_dir),\n",
    "        sample_edges=1000,        # For inspection only\n",
    "    )\n",
    "\n",
    "    # Compute metrics\n",
    "    stats = compute_graph_stats(edge_path=edge_path, num_nodes=N)\n",
    "\n",
    "    graph_summaries.append({\n",
    "        \"pkeep\": pkeep,\n",
    "        \"tau\": tau,\n",
    "        \"nodes\": stats[\"nodes\"],\n",
    "        \"edges\": stats[\"edges\"],\n",
    "        \"avg_degree\": stats[\"avg_degree\"],\n",
    "        \"median_degree\": stats[\"median_degree\"],\n",
    "        \"isolated_nodes\": stats[\"isolated_nodes\"],\n",
    "    })\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5.3 – Save results into a summary CSV\n",
    "# ----------------------------------------------\n",
    "\n",
    "graphs_df = pd.DataFrame(graph_summaries)\n",
    "graphs_df_path = GRAPH_DIR / \"graph_summaries.csv\"\n",
    "graphs_df.to_csv(graphs_df_path, index=False)\n",
    "\n",
    "print(\"\\n[Graph] Summary of constructed graphs:\")\n",
    "display(graphs_df)\n",
    "print(f\"[Graph] Saved summary to: {graphs_df_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3272d359",
   "metadata": {},
   "source": [
    "## Section 6 - Louvain communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a191da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAPH_DIR  : /home/zealot/DTU/Computational_Tools/outputs/graphs\n",
      "LOUVAIN_DIR: /home/zealot/DTU/Computational_Tools/outputs/louvain\n",
      "\n",
      "[Section 6] Loaded graph summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkeep</th>\n",
       "      <th>tau</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>isolated_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>148477</td>\n",
       "      <td>25347</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>148477</td>\n",
       "      <td>101171</td>\n",
       "      <td>1.362783</td>\n",
       "      <td>2.0</td>\n",
       "      <td>106717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>148477</td>\n",
       "      <td>186603</td>\n",
       "      <td>2.513561</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>148477</td>\n",
       "      <td>321786</td>\n",
       "      <td>4.334490</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pkeep   tau   nodes   edges  avg_degree  median_degree  isolated_nodes\n",
       "0   0.05  0.19  148477   25347    0.341427            1.0          129176\n",
       "1   0.10  0.21  148477  101171    1.362783            2.0          106717\n",
       "2   0.15  0.22  148477  186603    2.513561            2.0           92679\n",
       "3   0.25  0.23  148477  321786    4.334490            3.0           77706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Section 6] Running Louvain for all graphs...\n",
      "\n",
      "================================================================================\n",
      "[Louvain] Running for pkeep=0.050, tau≈0.190000\n",
      "[Louvain] Using tau=0.190 for filenames\n",
      "[Louvain] Edges file: /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/edges_tau0.190.tsv\n",
      "[Louvain] Output dir: /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p190\n",
      "[Louvain] Loading edge list from /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/edges_tau0.190.tsv (N=148477)\n",
      "[Louvain] Modularity = 0.9344\n",
      "[Louvain] Communities found = 133000\n",
      "[Louvain] Partition saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p190/louvain_partition.tsv\n",
      "[Louvain] Summary saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p190/louvain_summary.json\n",
      "\n",
      "================================================================================\n",
      "[Louvain] Running for pkeep=0.100, tau≈0.210000\n",
      "[Louvain] Using tau=0.210 for filenames\n",
      "[Louvain] Edges file: /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/edges_tau0.210.tsv\n",
      "[Louvain] Output dir: /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p210\n",
      "[Louvain] Loading edge list from /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p210/edges_tau0.210.tsv (N=148477)\n",
      "[Louvain] Modularity = 0.8684\n",
      "[Louvain] Communities found = 110964\n",
      "[Louvain] Partition saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p210/louvain_partition.tsv\n",
      "[Louvain] Summary saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p210/louvain_summary.json\n",
      "\n",
      "================================================================================\n",
      "[Louvain] Running for pkeep=0.150, tau≈0.220000\n",
      "[Louvain] Using tau=0.220 for filenames\n",
      "[Louvain] Edges file: /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/edges_tau0.220.tsv\n",
      "[Louvain] Output dir: /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p220\n",
      "[Louvain] Loading edge list from /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p220/edges_tau0.220.tsv (N=148477)\n",
      "[Louvain] Modularity = 0.8335\n",
      "[Louvain] Communities found = 96645\n",
      "[Louvain] Partition saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p220/louvain_partition.tsv\n",
      "[Louvain] Summary saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p220/louvain_summary.json\n",
      "\n",
      "================================================================================\n",
      "[Louvain] Running for pkeep=0.250, tau≈0.230000\n",
      "[Louvain] Using tau=0.230 for filenames\n",
      "[Louvain] Edges file: /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/edges_tau0.230.tsv\n",
      "[Louvain] Output dir: /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p230\n",
      "[Louvain] Loading edge list from /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p230/edges_tau0.230.tsv (N=148477)\n",
      "[Louvain] Modularity = 0.8043\n",
      "[Louvain] Communities found = 81233\n",
      "[Louvain] Partition saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p230/louvain_partition.tsv\n",
      "[Louvain] Summary saved to /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p230/louvain_summary.json\n",
      "\n",
      "[Louvain] Raw Louvain output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkeep</th>\n",
       "      <th>tau</th>\n",
       "      <th>modularity</th>\n",
       "      <th>num_communities</th>\n",
       "      <th>edges_file</th>\n",
       "      <th>out_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.934393</td>\n",
       "      <td>133000</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.868431</td>\n",
       "      <td>110964</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.833455</td>\n",
       "      <td>96645</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.804253</td>\n",
       "      <td>81233</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pkeep   tau  modularity  num_communities  \\\n",
       "0   0.05  0.19    0.934393           133000   \n",
       "1   0.10  0.21    0.868431           110964   \n",
       "2   0.15  0.22    0.833455            96645   \n",
       "3   0.25  0.23    0.804253            81233   \n",
       "\n",
       "                                          edges_file  \\\n",
       "0  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "1  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "2  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "3  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "\n",
       "                                             out_dir  \n",
       "0  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "1  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "2  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "3  /home/zealot/DTU/Computational_Tools/outputs/l...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Section 6] Combined graph quality + Louvain modularity:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkeep</th>\n",
       "      <th>tau</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>isolated_nodes</th>\n",
       "      <th>modularity</th>\n",
       "      <th>num_communities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>148477</td>\n",
       "      <td>25347</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129176</td>\n",
       "      <td>0.934393</td>\n",
       "      <td>133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>148477</td>\n",
       "      <td>101171</td>\n",
       "      <td>1.362783</td>\n",
       "      <td>2.0</td>\n",
       "      <td>106717</td>\n",
       "      <td>0.868431</td>\n",
       "      <td>110964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>148477</td>\n",
       "      <td>186603</td>\n",
       "      <td>2.513561</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92679</td>\n",
       "      <td>0.833455</td>\n",
       "      <td>96645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>148477</td>\n",
       "      <td>321786</td>\n",
       "      <td>4.334490</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77706</td>\n",
       "      <td>0.804253</td>\n",
       "      <td>81233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pkeep   tau   nodes   edges  avg_degree  median_degree  isolated_nodes  \\\n",
       "0   0.05  0.19  148477   25347    0.341427            1.0          129176   \n",
       "1   0.10  0.21  148477  101171    1.362783            2.0          106717   \n",
       "2   0.15  0.22  148477  186603    2.513561            2.0           92679   \n",
       "3   0.25  0.23  148477  321786    4.334490            3.0           77706   \n",
       "\n",
       "   modularity  num_communities  \n",
       "0    0.934393           133000  \n",
       "1    0.868431           110964  \n",
       "2    0.833455            96645  \n",
       "3    0.804253            81233  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Section 6] Saved summary to: /home/zealot/DTU/Computational_Tools/outputs/graph_quality_with_louvain.csv\n",
      "\n",
      "[Section 6] Selected τ by highest modularity:\n",
      "  pkeep        : 0.050\n",
      "  tau          : 0.190000\n",
      "  modularity   : 0.9344\n",
      "  communities  : 133000\n",
      "  avg_degree   : 0.341\n",
      "  isolated_nodes: 129176\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# SECTION 6 — Louvain clustering\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from arxiv_semantic_graph.graph_clustering import run_louvain\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.0 — Paths for graph data and Louvain output\n",
    "# ----------------------------------------------------\n",
    "GRAPH_DIR = OUTPUTS / \"graphs\"       # where edges_tau*.tsv and graph_summaries live\n",
    "LOUVAIN_DIR = OUTPUTS / \"louvain\"    # where Louvain results will be saved\n",
    "LOUVAIN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"GRAPH_DIR  :\", GRAPH_DIR)\n",
    "print(\"LOUVAIN_DIR:\", LOUVAIN_DIR)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.1 — Load graph summaries\n",
    "# ----------------------------------------------------\n",
    "graphs_path = GRAPH_DIR / \"graph_summaries.csv\"\n",
    "if not graphs_path.exists():\n",
    "    raise FileNotFoundError(f\"graph_summaries.csv not found at: {graphs_path}\")\n",
    "\n",
    "graphs_df = pd.read_csv(graphs_path)\n",
    "print(\"\\n[Section 6] Loaded graph summaries:\")\n",
    "display(graphs_df)\n",
    "\n",
    "# Required columns for clustering\n",
    "required_cols = {\"pkeep\", \"tau\", \"nodes\", \"edges\"}\n",
    "missing = required_cols - set(graphs_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in graph_summaries.csv: {missing}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.2 — Helper: run Louvain for a single row\n",
    "# ----------------------------------------------------\n",
    "def run_louvain_for_row(row) -> dict:\n",
    "    \"\"\"\n",
    "    Run Louvain clustering for a single graph configuration (one row of graphs_df).\n",
    "\n",
    "    The file naming convention must match Section 5:\n",
    "        Folder  : graphs/tau_0p190/\n",
    "        Edges   : graphs/tau_0p190/edges_tau0.190.tsv\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pandas.Series\n",
    "        Contains `pkeep`, `tau`, `nodes`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "        pkeep, tau, modularity, num_communities, paths\n",
    "    \"\"\"\n",
    "    pkeep = float(row[\"pkeep\"])\n",
    "    tau_raw = float(row[\"tau\"])\n",
    "    num_nodes = int(row[\"nodes\"])\n",
    "\n",
    "    # Round tau to 3 decimals for file naming consistency\n",
    "    tau_rounded = round(tau_raw, 3)\n",
    "    tau_tag = f\"tau_{tau_rounded:.3f}\".replace(\".\", \"p\")   # e.g. tau_0p190\n",
    "    tau_str = f\"{tau_rounded:.3f}\"                         # e.g. 0.190\n",
    "\n",
    "    # Construct file paths\n",
    "    edge_path = GRAPH_DIR / tau_tag / f\"edges_tau{tau_str}.tsv\"\n",
    "    out_dir = LOUVAIN_DIR / tau_tag\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[Louvain] Running for pkeep={pkeep:.3f}, tau≈{tau_raw:.6f}\")\n",
    "    print(f\"[Louvain] Using tau={tau_str} for filenames\")\n",
    "    print(f\"[Louvain] Edges file: {edge_path}\")\n",
    "    print(f\"[Louvain] Output dir: {out_dir}\")\n",
    "\n",
    "    # If the edge file does not exist, skip but include a result row\n",
    "    if not edge_path.exists():\n",
    "        print(f\"[WARNING] Edges not found, skipping: {edge_path}\")\n",
    "        return {\n",
    "            \"pkeep\": pkeep,\n",
    "            \"tau\": tau_raw,\n",
    "            \"modularity\": None,\n",
    "            \"num_communities\": None,\n",
    "            \"edges_file\": str(edge_path),\n",
    "            \"out_dir\": str(out_dir),\n",
    "        }\n",
    "\n",
    "    # Call the backend function from graph_clustering.py\n",
    "    res = run_louvain(\n",
    "        edge_path=str(edge_path),\n",
    "        num_nodes=num_nodes,\n",
    "        out_dir=str(out_dir),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pkeep\": pkeep,\n",
    "        \"tau\": tau_raw,\n",
    "        \"modularity\": res[\"modularity\"],\n",
    "        \"num_communities\": res[\"num_communities\"],\n",
    "        \"edges_file\": str(edge_path),\n",
    "        \"out_dir\": str(out_dir),\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.3 — Run Louvain for all tau values\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n[Section 6] Running Louvain for all graphs...\")\n",
    "\n",
    "louvain_results = [\n",
    "    run_louvain_for_row(row)\n",
    "    for _, row in graphs_df.iterrows()\n",
    "]\n",
    "\n",
    "louvain_df = pd.DataFrame(louvain_results)\n",
    "\n",
    "print(\"\\n[Louvain] Raw Louvain output:\")\n",
    "display(louvain_df)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.4 — Merge graph metrics + Louvain metrics\n",
    "# ----------------------------------------------------\n",
    "quality_df = graphs_df.merge(\n",
    "    louvain_df[[\"pkeep\", \"tau\", \"modularity\", \"num_communities\"]],\n",
    "    on=[\"pkeep\", \"tau\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(\"\\n[Section 6] Combined graph quality + Louvain modularity:\")\n",
    "display(quality_df)\n",
    "\n",
    "quality_path = OUTPUTS / \"graph_quality_with_louvain.csv\"\n",
    "quality_df.to_csv(quality_path, index=False)\n",
    "print(f\"[Section 6] Saved summary to: {quality_path}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6.5 — Select the “best” tau using modularity\n",
    "# ----------------------------------------------------\n",
    "valid_quality = quality_df.dropna(subset=[\"modularity\"])\n",
    "\n",
    "if valid_quality.empty:\n",
    "    print(\"\\n[Section 6] No valid modularity values — check graphs.\")\n",
    "else:\n",
    "    best_row = valid_quality.sort_values(\"modularity\", ascending=False).iloc[0]\n",
    "\n",
    "    print(\"\\n[Section 6] Selected τ by highest modularity:\")\n",
    "    print(f\"  pkeep        : {best_row['pkeep']:.3f}\")\n",
    "    print(f\"  tau          : {best_row['tau']:.6f}\")\n",
    "    print(f\"  modularity   : {best_row['modularity']:.4f}\")\n",
    "    print(f\"  communities  : {int(best_row['num_communities'])}\")\n",
    "\n",
    "    if \"avg_degree\" in best_row.index:\n",
    "        print(f\"  avg_degree   : {best_row['avg_degree']:.3f}\")\n",
    "    if \"isolated_nodes\" in best_row.index:\n",
    "        print(f\"  isolated_nodes: {int(best_row['isolated_nodes'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24bb94",
   "metadata": {},
   "source": [
    "## Section 7 - Recomendation demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1e8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Louvain] Available results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkeep</th>\n",
       "      <th>tau</th>\n",
       "      <th>modularity</th>\n",
       "      <th>num_communities</th>\n",
       "      <th>edges_file</th>\n",
       "      <th>out_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.934393</td>\n",
       "      <td>133000</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.868431</td>\n",
       "      <td>110964</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.833455</td>\n",
       "      <td>96645</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.804253</td>\n",
       "      <td>81233</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/g...</td>\n",
       "      <td>/home/zealot/DTU/Computational_Tools/outputs/l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pkeep   tau  modularity  num_communities  \\\n",
       "0   0.05  0.19    0.934393           133000   \n",
       "1   0.10  0.21    0.868431           110964   \n",
       "2   0.15  0.22    0.833455            96645   \n",
       "3   0.25  0.23    0.804253            81233   \n",
       "\n",
       "                                          edges_file  \\\n",
       "0  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "1  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "2  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "3  /home/zealot/DTU/Computational_Tools/outputs/g...   \n",
       "\n",
       "                                             out_dir  \n",
       "0  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "1  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "2  /home/zealot/DTU/Computational_Tools/outputs/l...  \n",
       "3  /home/zealot/DTU/Computational_Tools/outputs/l...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Selection] Best graph according to modularity:\n",
      "  pkeep         : 0.050\n",
      "  tau           : 0.190\n",
      "  modularity    : 0.9344\n",
      "  communities   : 133000\n",
      "\n",
      "[Selection] Resolved paths:\n",
      "  Graph dir   : /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190\n",
      "  Louvain dir : /home/zealot/DTU/Computational_Tools/outputs/louvain/tau_0p190\n",
      "  Edges file  : /home/zealot/DTU/Computational_Tools/outputs/graphs/tau_0p190/edges_tau0.190.tsv\n",
      "\n",
      "[Selection] Best graph configuration saved to: /home/zealot/DTU/Computational_Tools/outputs/best_graph_config.json\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# SECTION 7 — Select best graph (tau) from Louvain\n",
    "# ============================\n",
    "\n",
    "import json\n",
    "\n",
    "# 7.0 — Basic sanity checks\n",
    "print(\"\\n[Louvain] Available results:\")\n",
    "display(louvain_df)\n",
    "\n",
    "# Drop rows with missing modularity or trivial graphs (0 or 1 community)\n",
    "candidates = (\n",
    "    louvain_df\n",
    "    .dropna(subset=[\"modularity\"])\n",
    "    .query(\"num_communities > 1\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "if candidates.empty:\n",
    "    raise RuntimeError(\"[Louvain] No valid candidates to select from.\")\n",
    "\n",
    "# 7.1 — Select the graph with highest modularity\n",
    "candidates = candidates.sort_values(\"modularity\", ascending=False)\n",
    "best_row = candidates.iloc[0]\n",
    "\n",
    "BEST_PKEEP = float(best_row[\"pkeep\"])\n",
    "BEST_TAU = float(best_row[\"tau\"])\n",
    "BEST_MOD = float(best_row[\"modularity\"])\n",
    "BEST_COMM = int(best_row[\"num_communities\"])\n",
    "\n",
    "print(\"\\n[Selection] Best graph according to modularity:\")\n",
    "print(f\"  pkeep         : {BEST_PKEEP:.3f}\")\n",
    "print(f\"  tau           : {BEST_TAU:.3f}\")\n",
    "print(f\"  modularity    : {BEST_MOD:.4f}\")\n",
    "print(f\"  communities   : {BEST_COMM}\")\n",
    "\n",
    "# 7.2 — Reconstruct folder names (must match Section 5 & 6)\n",
    "tau_rounded = round(BEST_TAU, 3)\n",
    "tau_tag = f\"tau_{tau_rounded:.3f}\".replace(\".\", \"p\")   # e.g. tau_0p190\n",
    "tau_str = f\"{tau_rounded:.3f}\"                         # e.g. \"0.190\"\n",
    "\n",
    "BEST_GRAPH_DIR = GRAPH_DIR / tau_tag\n",
    "BEST_LOUVAIN_DIR = LOUVAIN_DIR / tau_tag\n",
    "BEST_EDGES_PATH = BEST_GRAPH_DIR / f\"edges_tau{tau_str}.tsv\"\n",
    "\n",
    "print(\"\\n[Selection] Resolved paths:\")\n",
    "print(f\"  Graph dir   : {BEST_GRAPH_DIR}\")\n",
    "print(f\"  Louvain dir : {BEST_LOUVAIN_DIR}\")\n",
    "print(f\"  Edges file  : {BEST_EDGES_PATH}\")\n",
    "\n",
    "# 7.3 — Save selection to a small JSON config for later use (e.g. in recommend.py)\n",
    "best_graph_config = {\n",
    "    \"pkeep\": BEST_PKEEP,\n",
    "    \"tau\": tau_rounded,\n",
    "    \"modularity\": BEST_MOD,\n",
    "    \"num_communities\": BEST_COMM,\n",
    "    \"graph_dir\": str(BEST_GRAPH_DIR),\n",
    "    \"louvain_dir\": str(BEST_LOUVAIN_DIR),\n",
    "    \"edges_path\": str(BEST_EDGES_PATH),\n",
    "}\n",
    "\n",
    "BEST_CONFIG_PATH = OUTPUTS / \"best_graph_config.json\"\n",
    "with BEST_CONFIG_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_graph_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n[Selection] Best graph configuration saved to: {BEST_CONFIG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a33179c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      " Random query paper\n",
      "============================\n",
      "ID    : 43034\n",
      "Year  : 2024\n",
      "Title : usfAD Based Effective Unknown Attack Detection Focused IDS Framework\n",
      "============================\n",
      "\n",
      "[Recommend] Loaded Louvain partition for 148,477 nodes.\n",
      "[Recommendations]\n",
      "- (id=1585, year=2024) ✓ significant\n",
      "    SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps\n",
      "    distance=0.1665\n",
      "\n",
      "- (id=13729, year=2024) ✓ significant\n",
      "    NAISR: A 3D Neural Additive Model for Interpretable Shape Representation\n",
      "    distance=0.1830\n",
      "\n",
      "- (id=53453, year=2024) ✓ significant\n",
      "    Structured Jet Model for Multiwavelength Observations of the Jetted\n",
      "    distance=0.1856\n",
      "\n",
      "- (id=60103, year=None) ✓ significant\n",
      "    stars with SHBoost\n",
      "    distance=0.1883\n",
      "\n",
      "- (id=6511, year=None) ✓ significant\n",
      "    Accelerator Technologies\n",
      "    distance=0.1904\n",
      "\n",
      "[Recommend] Loaded Louvain partition for 148,477 nodes.\n"
     ]
    }
   ],
   "source": [
    "from arxiv_semantic_graph import recommend\n",
    "import importlib\n",
    "importlib.reload(recommend)\n",
    "\n",
    "\n",
    "# Suppose:\n",
    "#   EMB_DIR = OUTPUTS / \"embeddings\"\n",
    "#   index   = your loaded hnswlib.Index\n",
    "\n",
    "recs = recommend.recommend_random(\n",
    "    emb_dir=str(EMB_DIR),\n",
    "    index=index,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "# Or for a specific id:\n",
    "recs_id42 = recommend.recommend_for_id(\n",
    "    emb_dir=str(EMB_DIR),\n",
    "    index=index,\n",
    "    paper_id=42,\n",
    "    k=5,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
